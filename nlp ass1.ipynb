{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "id": "af53yH39zL6C",
        "outputId": "44ea4542-1d9b-48a7-a4e0-ae29088867b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK - Tokenized: ['Two', 'days', 'ago', ',', 'at', 'midnight', ',', 'the', 'god', 'Apollo', 'had', 'visited', 'Lucia', '.', 'He', 'had', 'offered', 'her', 'a', 'choice', ':', 'gain', 'the', 'ability', 'to', 'see', 'what', 'will', 'happen', ',', 'but', 'lose', 'the', 'ability', 'to', 'lie', '.', 'So', ',', 'she', 'had', 'to', 'tell', 'her', 'master', 'everything', '.']\n",
            "spaCy - Tokenized: ['Two', 'days', 'ago', ',', 'at', 'midnight', ',', 'the', 'god', 'Apollo', 'had', 'visited', 'Lucia', '.', 'He', 'had', 'offered', 'her', 'a', 'choice', ':', 'gain', 'the', 'ability', 'to', 'see', 'what', 'will', 'happen', ',', 'but', 'lose', 'the', 'ability', 'to', 'lie', '.', 'So', ',', 'she', 'had', 'to', 'tell', 'her', 'master', 'everything', '.']\n",
            "\n",
            "NLTK - Lemmatized: ['Two', 'day', 'ago', ',', 'at', 'midnight', ',', 'the', 'god', 'Apollo', 'had', 'visited', 'Lucia', '.', 'He', 'had', 'offered', 'her', 'a', 'choice', ':', 'gain', 'the', 'ability', 'to', 'see', 'what', 'will', 'happen', ',', 'but', 'lose', 'the', 'ability', 'to', 'lie', '.', 'So', ',', 'she', 'had', 'to', 'tell', 'her', 'master', 'everything', '.']\n",
            "spaCy - Lemmatized: ['two', 'day', 'ago', ',', 'at', 'midnight', ',', 'the', 'god', 'Apollo', 'have', 'visit', 'Lucia', '.', 'he', 'have', 'offer', 'she', 'a', 'choice', ':', 'gain', 'the', 'ability', 'to', 'see', 'what', 'will', 'happen', ',', 'but', 'lose', 'the', 'ability', 'to', 'lie', '.', 'so', ',', 'she', 'have', 'to', 'tell', 'her', 'master', 'everything', '.']\n",
            "\n",
            "NLTK - Stopword Removed: ['Two', 'days', 'ago', ',', 'midnight', ',', 'god', 'Apollo', 'visited', 'Lucia', '.', 'offered', 'choice', ':', 'gain', 'ability', 'see', 'happen', ',', 'lose', 'ability', 'lie', '.', ',', 'tell', 'master', 'everything', '.']\n",
            "spaCy - Stopword Removed: ['days', 'ago', ',', 'midnight', ',', 'god', 'Apollo', 'visited', 'Lucia', '.', 'offered', 'choice', ':', 'gain', 'ability', 'happen', ',', 'lose', 'ability', 'lie', '.', ',', 'tell', 'master', '.']\n",
            "\n",
            "Two days ago - DATE\n",
            "\n",
            "midnight - TIME\n",
            "\n",
            "Apollo - ORG\n",
            "\n",
            "Lucia - PERSON\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "    <head>\n",
              "        <title>displaCy</title>\n",
              "    </head>\n",
              "\n",
              "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
              "<figure style=\"margin-bottom: 6rem\">\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Two days ago\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", at \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    midnight\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
              "</mark>\n",
              ", the god \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apollo\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " had visited \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Lucia\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ". He had offered her a choice: gain the ability to see what will happen, but lose the ability to lie. So, she had to tell her master everything.</div>\n",
              "</figure>\n",
              "</body>\n",
              "</html></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Embeddings for Tokens (BERT):\n",
            "tensor([[[-0.0774,  0.0195, -0.4101,  ...,  0.3049,  0.0917,  0.5279],\n",
            "         [ 0.3108, -0.3685, -0.2091,  ...,  0.1433,  0.5911,  0.1472],\n",
            "         [-0.0216, -0.3929,  0.1241,  ..., -0.4278, -0.2693, -0.3701],\n",
            "         ...,\n",
            "         [ 0.0381, -0.2653, -0.3706,  ...,  0.8441, -0.1329,  0.8563],\n",
            "         [-0.0147, -0.4700, -0.2509,  ...,  0.5974,  0.3383, -0.5641],\n",
            "         [-0.2832, -0.0934, -0.2271,  ...,  0.6953, -0.2552,  0.0585]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape of hidden states: torch.Size([1, 49, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentiment Analysis Result: [{'label': 'POSITIVE', 'score': 0.9994648098945618}]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import pipeline, BertTokenizer, BertModel\n",
        "import torch\n",
        "from spacy import displacy\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Text\n",
        "text = \"Two days ago, at midnight, the god Apollo had visited Lucia. He had offered her a choice: gain the ability to see what will happen, but lose the ability to lie. So, she had to tell her master everything.\"\n",
        "\n",
        "# Initialize NLTK tools\n",
        "nltk_lemmatizer = WordNetLemmatizer()\n",
        "nltk_stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Tokenize, Lemmatize, and Remove Stopwords using NLTK\n",
        "nltk_tokens = word_tokenize(text)\n",
        "nltk_lemmatized = [nltk_lemmatizer.lemmatize(token) for token in nltk_tokens]\n",
        "nltk_filtered = [token for token in nltk_tokens if token.lower() not in nltk_stopwords]\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenize, Lemmatize, and Remove Stopwords using spaCy\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "spacy_lemmatized = [token.lemma_ for token in doc]\n",
        "spacy_filtered = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "# Print Results\n",
        "print(\"NLTK - Tokenized:\", nltk_tokens)\n",
        "print(\"spaCy - Tokenized:\", spacy_tokens)\n",
        "\n",
        "print(\"\\nNLTK - Lemmatized:\", nltk_lemmatized)\n",
        "print(\"spaCy - Lemmatized:\", spacy_lemmatized)\n",
        "\n",
        "print(\"\\nNLTK - Stopword Removed:\", nltk_filtered)\n",
        "print(\"spaCy - Stopword Removed:\", spacy_filtered)\n",
        "\n",
        "# Named Entity Recognition (NER) с использованием spaCy\n",
        "#spacy_ner = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "#print(\"\\nNamed Entity Recognition (NER):\", spacy_ner)\n",
        "for ent in doc.ents:\n",
        "    print(f\"\\n{ent.text} - {ent.label_}\")\n",
        "\n",
        "# Визуализируем Named Entities с помощью displacy\n",
        "displacy.render(doc, style='ent', page=True)\n",
        "\n",
        "# Токенизация и извлечение эмбеддингов с использованием BERT (transformers)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Токенизация текста\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Извлекаем эмбеддинги для каждого токена\n",
        "embeddings = outputs.last_hidden_state\n",
        "print(\"\\nWord Embeddings for Tokens (BERT):\")\n",
        "print(embeddings)\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation (saves memory)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "print(\"\\nShape of hidden states:\", embeddings.shape)\n",
        "\n",
        "\n",
        "# Анализ настроений с использованием Hugging Face pipeline\n",
        "sentiment_analyzer = pipeline('sentiment-analysis')\n",
        "\n",
        "# Пример текста для анализа настроений\n",
        "sentiment_result = sentiment_analyzer(\"I love programming with Python!\")\n",
        "\n",
        "# Выводим результат анализа настроений\n",
        "print(\"\\nSentiment Analysis Result:\", sentiment_result)"
      ]
    }
  ]
}